{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "        \n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_features, input_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # A bunch of convolutions one after another\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, 4, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # FCN classification layer\n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN:\n",
    "    def __init__(self, device, lr=0.0002, lambda_cycle=10, lambda_identity=0.5):\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize generators and discriminators\n",
    "        self.G = Generator().to(device)  # X -> Y\n",
    "        self.F = Generator().to(device)  # Y -> X\n",
    "        self.D_X = Discriminator().to(device)\n",
    "        self.D_Y = Discriminator().to(device)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.criterion_GAN = nn.MSELoss()\n",
    "        self.criterion_cycle = nn.L1Loss()\n",
    "        self.criterion_identity = nn.L1Loss()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer_G = optim.Adam(\n",
    "            list(self.G.parameters()) + list(self.F.parameters()),\n",
    "            lr=lr, betas=(0.5, 0.999)\n",
    "        )\n",
    "        self.optimizer_D_X = optim.Adam(self.D_X.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.optimizer_D_Y = optim.Adam(self.D_Y.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        \n",
    "        \n",
    "\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "    \n",
    "    def train(self, dataloader, epochs):\n",
    "        self.lr_scheduler_G = optim.lr_scheduler.LambdaLR(self.optimizer_G, lr_lambda=LambdaLR(epochs, 0, epochs / 2).step)\n",
    "        self.lr_scheduler_D_X = optim.lr_scheduler.LambdaLR(self.optimizer_D_X, lr_lambda=LambdaLR(epochs, 0, epochs / 2).step)\n",
    "        self.lr_scheduler_D_Y = optim.lr_scheduler.LambdaLR(self.optimizer_D_Y, lr_lambda=LambdaLR(epochs, 0, epochs / 2).step)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Start epoch {epoch}\")\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                # Set model input\n",
    "                real_X = batch[\"X\"].to(self.device)\n",
    "                real_Y = batch[\"Y\"].to(self.device)\n",
    "                \n",
    "                print(real_X.shape, real_Y.shape)\n",
    "\n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones((real_X.size(0), 1, 30, 30), requires_grad=False).to(self.device)\n",
    "                fake = torch.zeros((real_X.size(0), 1, 30, 30), requires_grad=False).to(self.device)\n",
    "                \n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "                \n",
    "                print(\"Train generators\")\n",
    "                self.optimizer_G.zero_grad()\n",
    "                \n",
    "                # Identity loss\n",
    "                loss_id_X = self.criterion_identity(self.F(real_X), real_X)\n",
    "                loss_id_Y = self.criterion_identity(self.G(real_Y), real_Y)\n",
    "                # loss_identity = (loss_id_X + loss_id_Y) / 2\n",
    "                loss_identity = (loss_id_X + loss_id_Y) * 5 # https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/train#L107\n",
    "                \n",
    "                # GAN loss\n",
    "                fake_Y = self.G(real_X)\n",
    "                loss_GAN_G = self.criterion_GAN(self.D_Y(fake_Y), valid)\n",
    "                fake_X = self.F(real_Y)\n",
    "                loss_GAN_F = self.criterion_GAN(self.D_X(fake_X), valid)\n",
    "                # loss_GAN = (loss_GAN_G + loss_GAN_F) / 2\n",
    "                loss_GAN = loss_GAN_G + loss_GAN_F # https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/train#L115\n",
    "                \n",
    "                # Cycle loss\n",
    "                recov_X = self.F(fake_Y)\n",
    "                loss_cycle_X = self.criterion_cycle(recov_X, real_X)\n",
    "                recov_Y = self.G(fake_X)\n",
    "                loss_cycle_Y = self.criterion_cycle(recov_Y, real_Y)\n",
    "                # loss_cycle = (loss_cycle_X + loss_cycle_Y) / 2\n",
    "                loss_cycle = (loss_cycle_X + loss_cycle_Y) * 10 # https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/train#L123\n",
    "                \n",
    "                # Total loss\n",
    "                loss_G = loss_GAN + self.lambda_cycle * loss_cycle + self.lambda_identity * loss_identity\n",
    "                loss_G.backward()\n",
    "                self.optimizer_G.step()\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train Discriminator X\n",
    "                # ---------------------\n",
    "                print(\"train discriminators\")\n",
    "                self.optimizer_D_X.zero_grad()\n",
    "                \n",
    "                # Real loss\n",
    "                loss_real = self.criterion_GAN(self.D_X(real_X), valid)\n",
    "                # Fake loss\n",
    "                loss_fake = self.criterion_GAN(self.D_X(fake_X.detach()), fake)\n",
    "                # Total loss\n",
    "                loss_D_X = (loss_real + loss_fake) * 0.5 # use * 0.5 to make sure the result is float\n",
    "                loss_D_X.backward()\n",
    "                self.optimizer_D_X.step()\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train Discriminator Y\n",
    "                # ---------------------\n",
    "                \n",
    "                self.optimizer_D_Y.zero_grad()\n",
    "                \n",
    "                # Real loss\n",
    "                loss_real = self.criterion_GAN(self.D_Y(real_Y), valid)\n",
    "                # Fake loss\n",
    "                loss_fake = self.criterion_GAN(self.D_Y(fake_Y.detach()), fake)\n",
    "                # Total loss\n",
    "                loss_D_Y = (loss_real + loss_fake) * 0.5 # use * 0.5 to make sure the result is float\n",
    "                loss_D_Y.backward()\n",
    "                self.optimizer_D_Y.step()\n",
    "                \n",
    "                # Print progress\n",
    "                if i % 5 == 0:\n",
    "                    print(\n",
    "                        f\"[Epoch {epoch}/{epochs}] \"\n",
    "                        f\"[Batch {i}/{len(dataloader)}] \"\n",
    "                        f\"Loss D_X: {loss_D_X.item():.4f} \"\n",
    "                        f\"Loss D_Y: {loss_D_Y.item():.4f} \"\n",
    "                        f\"Loss G: {loss_G.item():.4f} \"\n",
    "                        f\"Loss GAN: {loss_GAN.item():.4f} \"\n",
    "                        f\"Loss cycle: {loss_cycle.item():.4f}\"\n",
    "                    )\n",
    "                \n",
    "                # Save images for visualization\n",
    "                if i % 500 == 0:\n",
    "                    self.save_images(epoch, i, real_X, real_Y, fake_X, fake_Y)\n",
    "    \n",
    "    def save_images(self, epoch, batch, real_X, real_Y, fake_X, fake_Y):\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        \n",
    "        # Save generated images\n",
    "        save_image(fake_X, f\"images/{epoch}_{batch}_fake_X.png\", normalize=True)\n",
    "        save_image(fake_Y, f\"images/{epoch}_{batch}_fake_Y.png\", normalize=True)\n",
    "        save_image(real_X, f\"images/{epoch}_{batch}_real_X.png\", normalize=True)\n",
    "        save_image(real_Y, f\"images/{epoch}_{batch}_real_Y.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_X, root_Y, transform=None, unaligned=False):\n",
    "        self.transform = transforms.Compose(transform)\n",
    "        self.unaligned = unaligned\n",
    "        \n",
    "        self.files_X = sorted([os.path.join(root_X, f) for f in os.listdir(root_X)])\n",
    "        self.files_Y = sorted([os.path.join(root_Y, f) for f in os.listdir(root_Y)])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item_X = self.transform(Image.open(self.files_X[index % len(self.files_X)]))\n",
    "        \n",
    "        if self.unaligned:\n",
    "            item_Y = self.transform(Image.open(self.files_Y[random.randint(0, len(self.files_Y) - 1)]))\n",
    "        else:\n",
    "            item_Y = self.transform(Image.open(self.files_Y[index % len(self.files_Y)]))\n",
    "        \n",
    "        return {\"X\": item_X, \"Y\": item_Y}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_X), len(self.files_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "batch_size = 4\n",
    "image_size = 256\n",
    "\n",
    "# Transforms\n",
    "transform = [\n",
    "    transforms.Resize(int(image_size * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]\n",
    "\n",
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"../data/photo_jpg\", \"../data/monet_jpg\", transform=transform, unaligned=True),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "cyclegan = CycleGAN(device)\n",
    "\n",
    "# Train\n",
    "cyclegan.train(dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
